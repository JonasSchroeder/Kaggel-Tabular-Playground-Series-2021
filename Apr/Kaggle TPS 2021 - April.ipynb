{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5e41ac",
   "metadata": {},
   "source": [
    "### Kaggle note for this TPS\n",
    "\"The dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN. The statistical properties of this dataset are very similar to the original Titanic dataset, but there's no way to \"cheat\" by using public labels for predictions. How well does your model perform on truly unseen data?\"\n",
    "\n",
    "Idea: Start with my existing model used for the Titanic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "784e1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"input/train.csv\")\n",
    "\n",
    "submit_df = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9cb23d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7cac3c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Oconnor, Frankie</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>209245</td>\n",
       "      <td>27.14</td>\n",
       "      <td>C12239</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Bryan, Drew</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27323</td>\n",
       "      <td>13.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Owens, Kenneth</td>\n",
       "      <td>male</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA 457703</td>\n",
       "      <td>71.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kramer, James</td>\n",
       "      <td>male</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A. 10866</td>\n",
       "      <td>13.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Bond, Michael</td>\n",
       "      <td>male</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>427635</td>\n",
       "      <td>7.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass              Name   Sex    Age  SibSp  Parch  \\\n",
       "0            0         1       1  Oconnor, Frankie  male    NaN      2      0   \n",
       "1            1         0       3       Bryan, Drew  male    NaN      0      0   \n",
       "2            2         0       3    Owens, Kenneth  male   0.33      1      2   \n",
       "3            3         0       3     Kramer, James  male  19.00      0      0   \n",
       "4            4         1       3     Bond, Michael  male  25.00      0      0   \n",
       "\n",
       "      Ticket   Fare   Cabin Embarked  \n",
       "0     209245  27.14  C12239        S  \n",
       "1      27323  13.35     NaN        S  \n",
       "2  CA 457703  71.29     NaN        S  \n",
       "3   A. 10866  13.04     NaN        S  \n",
       "4     427635   7.76     NaN        S  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc807dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   PassengerId  100000 non-null  int64  \n",
      " 1   Survived     100000 non-null  int64  \n",
      " 2   Pclass       100000 non-null  int64  \n",
      " 3   Name         100000 non-null  object \n",
      " 4   Sex          100000 non-null  object \n",
      " 5   Age          96708 non-null   float64\n",
      " 6   SibSp        100000 non-null  int64  \n",
      " 7   Parch        100000 non-null  int64  \n",
      " 8   Ticket       95377 non-null   object \n",
      " 9   Fare         99866 non-null   float64\n",
      " 10  Cabin        32134 non-null   object \n",
      " 11  Embarked     99750 non-null   object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c398ba2",
   "metadata": {},
   "source": [
    "# 1. Data Transformation and Pre-Processing\n",
    "\n",
    "First: transform and then drop rows with missing values (simple version, see how it goes).\n",
    "\n",
    "Later: See if we can improve the model by inference and filling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1547aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DATA\n",
    "\n",
    "import re\n",
    "\n",
    "# Rename class\n",
    "train_df[\"Pclass\"].replace(1, \"Upper\", inplace=True)\n",
    "train_df[\"Pclass\"].replace(2, \"Middle\", inplace=True)\n",
    "train_df[\"Pclass\"].replace(3, \"Lower\", inplace=True)\n",
    "\n",
    "# Replace missing age with median -> age inference method?\n",
    "train_df[\"Age\"].fillna(np.nanmedian(train_df[\"Age\"]), inplace=True)\n",
    "\n",
    "# Create Age Bands (thinner age bands didn't improve the model)\n",
    "bins = [0,10,20,30,40,50,60,70,100]\n",
    "train_df[\"Age_bin\"] = pd.cut(train_df['Age'], bins)\n",
    "\n",
    "# Replace Fare==0 with median\n",
    "# train_df[\"Fare\"].replace(0, np.median(train_df[\"Fare\"]), inplace=True)\n",
    "\n",
    "# Create Fare Bands -> did not improve the model\n",
    "# bins = [0,50,100,200,500,1000]\n",
    "# train_df[\"Fare_bin\"] = pd.cut(train_df['Fare'], bins)\n",
    "\n",
    "# With family \n",
    "train_df[\"with_family\"] = (train_df[\"SibSp\"] + train_df[\"Parch\"])>0\n",
    "\n",
    "# Replace NA for embarked with \"S\"\n",
    "train_df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "\n",
    "# Replace NA for Cabin with \"Unknown\"\n",
    "train_df[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Extract Name Title\n",
    "#train_df[\"Title\"] = train_df.Name.apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\n",
    "#title_list = [\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Col\"]\n",
    "#train_df.loc[~train_df[\"Title\"].isin(title_list), \"Title\"] = \"NA\"\n",
    "\n",
    "# Extract deck from Cabin\n",
    "for i in range(0, len(train_df)):\n",
    "    train_df.at[i, \"Deck\"] = \" \".join(re.findall(\"[a-zA-Z]+\", train_df.at[i, \"Cabin\"]))\n",
    "\n",
    "train_df[\"Deck\"].replace(\"B B\", \"B\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"B B B\", \"B\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"B B B B\", \"B\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"C C\", \"C\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"D D\", \"D\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"C C C\", \"C\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"F G\", \"F\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"F E\", \"E\", inplace=True)\n",
    "train_df[\"Deck\"].replace(\"T\", \"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7cefa5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08e2dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95249 entries, 0 to 99999\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   PassengerId  95249 non-null  int64   \n",
      " 1   Survived     95249 non-null  int64   \n",
      " 2   Pclass       95249 non-null  object  \n",
      " 3   Name         95249 non-null  object  \n",
      " 4   Sex          95249 non-null  object  \n",
      " 5   Age          95249 non-null  float64 \n",
      " 6   SibSp        95249 non-null  int64   \n",
      " 7   Parch        95249 non-null  int64   \n",
      " 8   Ticket       95249 non-null  object  \n",
      " 9   Fare         95249 non-null  float64 \n",
      " 10  Cabin        95249 non-null  object  \n",
      " 11  Embarked     95249 non-null  object  \n",
      " 12  Age_bin      95249 non-null  category\n",
      " 13  with_family  95249 non-null  bool    \n",
      " 14  Deck         95249 non-null  object  \n",
      "dtypes: bool(1), category(1), float64(2), int64(4), object(7)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6544064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMIT DATA\n",
    "\n",
    "# Rename class\n",
    "submit_df[\"Pclass\"].replace(1, \"Upper\", inplace=True)\n",
    "submit_df[\"Pclass\"].replace(2, \"Middle\", inplace=True)\n",
    "submit_df[\"Pclass\"].replace(3, \"Lower\", inplace=True)\n",
    "\n",
    "# Replace missing age with median\n",
    "submit_df[\"Age\"].fillna(np.nanmedian(submit_df[\"Age\"]), inplace=True)\n",
    "\n",
    "# Create Age Bands\n",
    "bins = [0,10,20,30,40,50,60,70,100]\n",
    "submit_df[\"Age_bin\"] = pd.cut(submit_df['Age'], bins)\n",
    "\n",
    "# Replace missing fare with median\n",
    "submit_df[\"Fare\"].fillna(np.nanmedian(submit_df[\"Fare\"]), inplace=True)\n",
    "\n",
    "# Replace Fare==0 with median\n",
    "# test_df[\"Fare\"].replace(0, np.median(test_df[\"Fare\"]), inplace=True)\n",
    "\n",
    "# Create Fare Bands -> did not improve the model\n",
    "# bins = [0,50,100,200,500,1000]\n",
    "# test_df[\"Fare_bin\"] = pd.cut(test_df['Fare'], bins)\n",
    "\n",
    "# Replace NA for embarked with \"S\"\n",
    "submit_df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "\n",
    "# Replace NA for Cabin with \"Unknown\"\n",
    "submit_df[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# With family\n",
    "submit_df[\"with_family\"] = (submit_df[\"SibSp\"] + submit_df[\"Parch\"])>0\n",
    "\n",
    "# Extract Name Title\n",
    "#submit_df[\"Title\"] = submit_df.Name.apply(lambda x: x.split(\",\")[1].split(\".\")[0].strip())\n",
    "#title_list = [\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Col\"]\n",
    "#submit_df.loc[~submit_df[\"Title\"].isin(title_list), \"Title\"] = \"NA\"\n",
    "\n",
    "# Extract deck from Cabin\n",
    "for i in range(0, len(submit_df)):\n",
    "    submit_df.at[i, \"Deck\"] = \" \".join(re.findall(\"[a-zA-Z]+\", submit_df.at[i, \"Cabin\"]))\n",
    "\n",
    "submit_df[\"Deck\"].replace(\"B B\", \"B\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"B B B\", \"B\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"B B B B\", \"B\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"C C\", \"C\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"E E\", \"E\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"D D\", \"D\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"C C C\", \"C\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"F G\", \"F\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"F E\", \"E\", inplace=True)\n",
    "submit_df[\"Deck\"].replace(\"T\", \"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e5d4e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df[\"Ticket\"]=submit_df[\"Ticket\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20d9d201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count   Dtype   \n",
      "---  ------       --------------   -----   \n",
      " 0   PassengerId  100000 non-null  int64   \n",
      " 1   Pclass       100000 non-null  object  \n",
      " 2   Name         100000 non-null  object  \n",
      " 3   Sex          100000 non-null  object  \n",
      " 4   Age          100000 non-null  float64 \n",
      " 5   SibSp        100000 non-null  int64   \n",
      " 6   Parch        100000 non-null  int64   \n",
      " 7   Ticket       100000 non-null  object  \n",
      " 8   Fare         100000 non-null  float64 \n",
      " 9   Cabin        100000 non-null  object  \n",
      " 10  Embarked     100000 non-null  object  \n",
      " 11  Age_bin      100000 non-null  category\n",
      " 12  with_family  100000 non-null  bool    \n",
      " 13  Deck         100000 non-null  object  \n",
      "dtypes: bool(1), category(1), float64(2), int64(3), object(7)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "submit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b7e92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Standardization and One-Hot Encoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "train_df_pre = train_df.drop(columns=[\"Name\", \"Ticket\", \"Cabin\"])\n",
    "submit_df_pre = submit_df.drop(columns=[ \"Name\", \"Ticket\", \"Cabin\"])\n",
    "\n",
    "#train_df_pre = train_df.drop(columns=[\"Age\", \"Fare\", \"Name\", \"Ticket\", \"Cabin\"])\n",
    "#test_df_pre = test_df.drop(columns=[\"Age\", \"Fare\", \"Name\", \"Ticket\", \"Cabin\"])\n",
    "\n",
    "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "#num_attribs = [\"SibSp\", \"Parch\",]\n",
    "cat_attribs = [\"Pclass\", \"Embarked\", \"Deck\", \"Sex\", \"with_family\", \"Age_bin\"]#, \"Fare_bin\"]\n",
    "\n",
    "col_transformer = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ],\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "# Fit transform TRAIN\n",
    "train_array_transformed = col_transformer.fit_transform(train_df_pre)\n",
    "\n",
    "# Convert numpy.ndarray to pd.DataFrame\n",
    "train_df_transformed = pd.DataFrame(data=train_array_transformed)\n",
    "#train_df_transformed = pd.DataFrame(data=train_array_transformed.toarray())\n",
    "\n",
    "# Rename columns\n",
    "column_names = num_attribs + list(col_transformer.named_transformers_['cat'].get_feature_names()) + [\"PassengerId\"] + [\"Survived\"]\n",
    "train_df_transformed.columns = column_names\n",
    "\n",
    "\n",
    "# Fit transform SUBMIT\n",
    "submit_array_transformed = col_transformer.fit_transform(submit_df_pre)\n",
    "\n",
    "# Convert numpy.ndarray to pd.DataFrame\n",
    "submit_df_transformed = pd.DataFrame(data=submit_array_transformed)\n",
    "#test_df_transformed = pd.DataFrame(data=test_array_transformed.toarray())\n",
    "\n",
    "# Rename columns\n",
    "column_names = num_attribs + list(col_transformer.named_transformers_['cat'].get_feature_names()) + [\"PassengerId\"]\n",
    "submit_df_transformed.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "67fd816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'SibSp', 'Parch', 'Fare', 'x0_Lower', 'x0_Middle', 'x0_Upper',\n",
      "       'x1_C', 'x1_Q', 'x1_S', 'x2_A', 'x2_B', 'x2_C', 'x2_D', 'x2_E', 'x2_F',\n",
      "       'x2_G', 'x2_Unknown', 'x3_female', 'x3_male', 'x4_False', 'x4_True',\n",
      "       'x5_(0, 10]', 'x5_(10, 20]', 'x5_(20, 30]', 'x5_(30, 40]',\n",
      "       'x5_(40, 50]', 'x5_(50, 60]', 'x5_(60, 70]', 'x5_(70, 100]',\n",
      "       'PassengerId', 'Survived'],\n",
      "      dtype='object')\n",
      "Index(['Age', 'SibSp', 'Parch', 'Fare', 'x0_Lower', 'x0_Middle', 'x0_Upper',\n",
      "       'x1_C', 'x1_Q', 'x1_S', 'x2_A', 'x2_B', 'x2_C', 'x2_D', 'x2_E', 'x2_F',\n",
      "       'x2_G', 'x2_Unknown', 'x3_female', 'x3_male', 'x4_False', 'x4_True',\n",
      "       'x5_(0, 10]', 'x5_(10, 20]', 'x5_(20, 30]', 'x5_(30, 40]',\n",
      "       'x5_(40, 50]', 'x5_(50, 60]', 'x5_(60, 70]', 'x5_(70, 100]',\n",
      "       'PassengerId'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "print(train_df_transformed.columns)\n",
    "print(submit_df_transformed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1c301",
   "metadata": {},
   "source": [
    "# 2. Model Building and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65c975",
   "metadata": {},
   "source": [
    "### 1. Random Forest without Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96acd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df_transformed.drop(columns=[\"Survived\", \"PassengerId\"])\n",
    "y = train_df_transformed[[\"Survived\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8e125ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest simple\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forrest classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3abdb610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.994435612082671\n",
      "test score: 0.9957305336832896\n"
     ]
    }
   ],
   "source": [
    "print(\"train score: \" + str(rf.score(X_train, y_train)))\n",
    "print(\"test score: \" + str(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "90048089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   14.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Random Forest tuned\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forrest classifier\n",
    "rf = RandomForestClassifier(verbose=1)\n",
    "\n",
    "# Find optimal parameter settings using Randomized Search\n",
    "param_grid =  {'n_estimators': [200,300,400,600,800,1000], \n",
    "               'bootstrap': [True,False],\n",
    "               'max_depth': [3,5,10,20,50,None],\n",
    "               'max_features': ['auto'],\n",
    "               'min_samples_leaf': [1,2,4,6,10],\n",
    "               'min_samples_split': [2,5,10,20]}\n",
    "\n",
    "rnd_clf = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=300, cv=3, verbose=5, n_jobs=-1)\n",
    "\n",
    "best_rnd_clf = rnd_clf.fit(X, np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "054ec81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=300,\n",
       "                       verbose=1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnd_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8da486c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7914029456759757\n",
      "test score: 0.7974103237095364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"train score: \" + str(best_rnd_clf.score(X_train, y_train)))\n",
    "print(\"test score: \" + str(best_rnd_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "58a8a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Export for submit\n",
    "import datetime as datetime\n",
    "export_df = pd.DataFrame()\n",
    "export_df[\"PassengerId\"] = submit_df_transformed[\"PassengerId\"].astype(int)\n",
    "export_df[\"Survived\"] = best_rnd_clf.predict(submit_df_transformed.drop(columns=[\"PassengerId\"])).astype(int)\n",
    "now = datetime.datetime.now()\n",
    "name_add = \"date_\"+str(now.year)+\"-\"+str(now.month)+\"-\"+str(now.day)+\"_time_\"+str(now.hour)+\"-\"+str(now.minute)\n",
    "export_df.to_csv(f\"output/random_forest_tuned_{name_add}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ce4a8",
   "metadata": {},
   "source": [
    "### 2. Random Forest with Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a59b1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df_transformed.drop(columns=[\"Survived\", \"PassengerId\"])\n",
    "y = train_df_transformed[[\"Survived\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "61db13d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest simple\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forrest classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7342c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9956204817470078\n",
      "test score: 0.735573053368329\n"
     ]
    }
   ],
   "source": [
    "print(\"train score: \" + str(rf.score(X_train, y_train)))\n",
    "print(\"test score: \" + str(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "245a664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    7.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Random Forest tuned\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forrest classifier\n",
    "rf = RandomForestClassifier(verbose=1)\n",
    "\n",
    "# Find optimal parameter settings using Randomized Search\n",
    "param_grid =  {'n_estimators': [200,300,400,600,800,1000], \n",
    "               'bootstrap': [True,False],\n",
    "               'max_depth': [3,5,10,20,50,None],\n",
    "               'max_features': ['auto'],\n",
    "               'min_samples_leaf': [1,2,4,6,10],\n",
    "               'min_samples_split': [2,5,10,20]}\n",
    "\n",
    "rnd_clf = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=300, cv=3, verbose=5, n_jobs=-1)\n",
    "\n",
    "best_rnd_clf = rnd_clf.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dbffcb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_leaf=6, min_samples_split=5,\n",
       "                       n_estimators=300, verbose=1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnd_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ca68a331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7915829258781534\n",
      "test score: 0.7899912510936133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"train score: \" + str(best_rnd_clf.score(X_train, y_train)))\n",
    "print(\"test score: \" + str(best_rnd_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30fccdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Export for submit\n",
    "import datetime as datetime\n",
    "export_df = pd.DataFrame()\n",
    "export_df[\"PassengerId\"] = submit_df_transformed[\"PassengerId\"].astype(int)\n",
    "export_df[\"Survived\"] = best_rnd_clf.predict(submit_df_transformed.drop(columns=[\"PassengerId\"])).astype(int)\n",
    "now = datetime.datetime.now()\n",
    "name_add = \"date_\"+str(now.year)+\"-\"+str(now.month)+\"-\"+str(now.day)+\"_time_\"+str(now.hour)+\"-\"+str(now.minute)\n",
    "export_df.to_csv(f\"output/random_forest_tuned_{name_add}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe2cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
